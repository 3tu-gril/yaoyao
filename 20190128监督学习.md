# 1.5 正则化与交叉验证
## 1.5.1 正则化
* 正则化（regularization)：模型选择的典型方法,是在经验风险上加一个正则化项（regularizer)或罚项(penalty term)
## 1.5.2 交叉验证（cross validation)
* 如果给定的样本数据充分，进行模型选择的一种方法是随机的将数据集切分成三部分，分别为训练集，验证集，测试集，训练集用来训练模型，验证集用于模型的选择，测试集用于最终对学习方法的评估。
* 用于在实际中数据是不充足的，为了选择好的模型，可以使用交叉验证方法。
1 简单交叉验证:
首先随机的将已给数据分为两部分，一部分作为训练集，一部分作为测试集，（例如70%的数据作为训练集，30%的数据作为测试集）然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型，在测试集上评价各个模型的测试误差，选出测试误差最小的模型。
2 $S$折交叉验证
应用最多的是$S$折交叉验证，首先随机地将已给数据切分为$S$个互不相交的大小相同的子集，然后利用$S$-1个子集的数据训练模型，利用余下的子集测试模型，将这一个过程对可能的$S$种选择重复进行，最后选出$S$次评测中平均测试误差最小的模型。
3 留一交叉验证
$S$折交叉验证的特殊情形是$S$=$N$,称为留一交叉验证（leave-one-out cross validation),往往在数据缺乏的时候使用（$N$是给定数据集的容量)
# 1.6 泛化能力
## 1.6.1 泛化能力
学习方法的泛化能力是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。
* 通常采用测试误差来评价学习方法的泛化能力。但是因为测试数据集是有限的，很有可能由此得到的评价结果是不可靠的。
# 1.7 生成模型与判别模型
监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出，这个模型的一般形式为决策函数：
$Y=f(X)$
或者条件概率分布：
$P(Y|X)$
* 生成模型和判别模型
监督学习的方法可以成为生成方法（generative approach)和判别方法(discriminative approach),所以所学到的模型分别称为生成模型（generative model)和判别模型(discriminative model)。
* 1 生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：
$P(Y|X)$=$\frac{P(X,Y)}{P(X)}$
* 这种方法之所以称为生成方法，是因为模型表示了给定输入$X$产生输入$Y$的生成关系。典型的生成模型有：
     * 朴素贝叶斯
     * 隐马尔科夫模型
* 2 判别模型：判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。
* 判别方法关心的是对给定的输入$X$，应预测什么的样的输出$Y$,典型的判别方法有：
  * k 近邻
  * 感知机
  * 决策树
  * 逻辑斯蒂回归模型
  * 最大熵模型
  * 支持向量机
  * 提升方法
  * 条件随机场
  * 3 两者的特点比较：
* 生成方法：
  * 1 可以还原出联合概率分布$P(Y|X)$
  * 2 学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型
  * 3 当存在隐变量时，仍然可以用生成方法进行学习，此时，判别方法将不能使用。
* 判别方法：
  * 1 判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测， 往往学习的效率更高
  * 2 由于直接学习概率$P(Y|X)$或决策函数$f(X)$，可以对数据进行各种程度上的抽象，定义特征并使用特征， 因此可以简化学习问题。
  # 1.8 分类问题
  分类问题是监督学习的一个重要问题，分类问题的特点是：输出变量是离散型变量，但输入变量可以是连续的也可以是离散的。
  * 分类器：监督学习从数据中学习一个分类模型或分类决策函数，成为分类器。
  * 分类：分类器对新的输入进行输出的预测，称为分类，可能的输出称为类（class）。
  分类的类别为多个时，称为多类分类问题。
  * 分类问题包括学习和分类两个过程，
      * 学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器。
      * 分类过程中，利用学习的分类器对新的输入实例进行分类。
* 二分类问题常用的评价指标是精确率（precision）与召回率(recall)，通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确不正确，4种情况出现的总数分别记作：
   * $TP$ ：将正类预测为正类数
   * $FN$ : 将正类预测为负类数
   * $FP$ : 将负类预测为正类数
  * $TN$ : 将负类预测为负类数
（其中P（positive)表示的是正类数，N(negative)表示的是负类数）（T(ture),F(false))
 * 精确率定义为：
 $P$=$\frac{TP}{TP+FP}$
  * 召回率定义为：
  $R$=$\frac{TP}{TP+FN}$
  # 1.9 标注问题
  标注问题是分类问题的一个推广，标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。
标注常用的统计学习方法有：隐马尔可夫模型，条件随机场
标注问题在信息抽取，自然语言处理等领域被广泛使用。
# 1.10 回归问题（regression）
回归模型是表示从输入变量到输出变量之间映射的函数，
* 回归问题的学习等价于函数拟合，选择一条函数曲线，使其很好的拟合已知数据且很好的预测未知数据。
* 回归问题分为学习和预测两个过程，通过对已有数据的学习而获得一个模型，然后通过这个模型对新数据进行预测。
* 按照输入变量的个数可以分为：
    * 一元回归和多元回归
* 按照输入变量和输出变量之间关系的类型也就模型的类型，可以分为线性回归和非线性回归。
* 回归学习最常用的损失函数是平方损失函数，因而回归问题可以由最小二乘法求解。



  






